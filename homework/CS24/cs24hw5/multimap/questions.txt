Multimap Caching Performance
============================

b)  Output of mmperf:
Testing multimap performance:  300000 pairs, 1000000 probes, random keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997756/1000000 (99.8%)
Total wall-clock time:  13.97 seconds       us per probe:  13.965 us

Testing multimap performance:  300000 pairs, 1000000 probes, incrementing keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997309/1000000 (99.7%)
Total wall-clock time:  10.65 seconds       us per probe:  10.645 us

Testing multimap performance:  300000 pairs, 1000000 probes, decrementing keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997282/1000000 (99.7%)
Total wall-clock time:  10.97 seconds       us per probe:  10.968 us

Testing multimap performance:  15000000 pairs, 1000000 probes, random keys.
Adding 15000000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 1000000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  950564/1000000 (95.1%)
Total wall-clock time:  4.73 seconds        us per probe:  4.733 us

Testing multimap performance:  100000 pairs, 50000 probes, incrementing keys.
Adding 100000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 50000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  955/50000 (1.9%)
Total wall-clock time:  11.61 seconds       us per probe:  232.169 us

Testing multimap performance:  100000 pairs, 50000 probes, decrementing keys.
Adding 100000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 50000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  964/50000 (1.9%)
Total wall-clock time:  11.79 seconds       us per probe:  235.769 us


c)  Explanation of tests:
tests 1 - 3:
These tests have a large number of pairs, but a small possible number of keys.
This results in each of the keys having many values, hence testing how well the
cache is used to optimize searching through the list of values attached to each
key.

tests 4 - 6:
These tests have a large number of keys. So, this tests how well the cache is
used to optimize searching through the map for the correct key.


e)  Explanation of your optimizations:

Overall idea:
To make better use of the cache, anytime a long list of things is being searched
through, that list of things should be contiguous in memory, and in the same
order (or as close to the same order as possible) as they will be searched
through in. This locality makes sure the cache loads in things that have the
highest probability of being required at the same time.

First change:
Linked lists, where nodes are added as needed, result in the values of the list
being spread throughout the memory, as each is a separate malloc. Since the
cache loads blocks of memory in at a time, it is very likely that searching
through a linked list will result in many misses as the memory locations will be
random. Originally, the multimap_values were stored in a linked list. Using an
array of values results in the values being next to each other in memory, so
this will make better use of the cache. Since the arrays must be variable length
as pairs can be added and removed, the value_list structure and accompanying
helper functions were made. This change resulted in the first three tests being
about 20x faster, fourth test being about 10x faster, and the last two being
the same speed.

Second change:
The same way the value lists can be optimized, the way the nodes themselves are
stored can be make better use of the cache. The current structure is
effectively stored as a linked list where each node has two children. Search
speed is faster due to insertion requirements. But, as seen in the last change,
linked lists do not make good use of the cache. So, the node structure can be
changed to an array. Since this keeps the data contiguous, it makes better use
of the cache. All the keys are integers and we do not know the range in which
they will be, so we will start with a small initial size, and at least double it
if a new key comes that is larger (more than double if neccessary). Then, that
many spaces for nodes will be allocated, and given a key k, it can be found by
accessing the kth index in the allocated space, and resizing the allocated space
or creating a new node as needed.

f)  Output of ommperf:



